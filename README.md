# Exponential Family Embeddings - Reproduction Study

A PyTorch reproduction of **Exponential Family Embeddings (EFE)** from Rudolph et al. (2016), focusing on Poisson-based models for count data applications.

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)

## ğŸ“– Overview

This project reproduces the Exponential Family Embeddings framework for learning distributed representations from count data. We implement three Poisson embedding variants and evaluate them on market basket and movie rating datasets.

### Models Implemented

| Model | Description | Mean Function |
|-------|-------------|---------------|
| **P-EMB** | Poisson Embedding (multiplicative) | Î» = exp(Ïáµ€á¾±) |
| **P-EMB-DW** | P-EMB with downweighted zeros (wâ‚€=0.1) | Î» = exp(Ïáµ€á¾±) |
| **AP-EMB** | Additive Poisson Embedding | Î» = Ïáµ€á¾± + b |
| **HPF** | Hierarchical Poisson Factorization (baseline) | Î» = Î¸áµ¤áµ€Î²áµ¢ |
| **Poisson PCA** | Poisson PCA (baseline) | Î» = exp(wáµ€h + c + Î¼) |

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/exponential-family-embeddings.git
cd exponential-family-embeddings

# Create virtual environment (optional but recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Requirements

```txt
torch>=2.0.0
numpy>=1.21.0
pandas>=1.3.0
scikit-learn>=1.0.0
matplotlib>=3.5.0
seaborn>=0.11.0
tqdm>=4.62.0
umap-learn>=0.5.0
```

### Run on Google Colab

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1l7fjNNbNhtjLGaWbSHo7st7mKQRJGxcd?usp=sharing)

1. Open the notebook in Google Colab
2. Go to `Runtime â†’ Change runtime type â†’ GPU`
3. Run all cells

## ğŸ“ Project Structure

```
exponential-family-embeddings/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Market_Basekt_Data_Analysis.ipynb    # Main notebook (Colab-ready)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ data.csv.zip              # raw market basket dataset
â””â”€â”€ results/
    â””â”€â”€ png files               # Generated plots and visualizations
```

## ğŸ“Š Datasets

### Market Basket Data

The dataset should contain grocery transaction records with the following columns:
Download from [Dunnhumby Data](https://www.dunnhumby.com/source-files/):

| Column | Description |
|--------|-------------|
| `product_id` | Unique product identifier |
| `household_key` | Household identifier |
| `basket_id` | Transaction/basket identifier |
| `quantity` | Purchase quantity |

### MovieLens Data

Download from [MovieLens](https://grouplens.org/datasets/movielens/):
- Ratings â‰¥ 3 are converted to counts
- Ratings < 3 are set to zero

## ğŸ“ˆ Results

### Market Basket Dataset

| Model | K=10 | K=20 | K=50 |
|-------|------|------|------|
| P-EMB | -7.30 Â± 0.010 | **-7.28 Â± 0.014** | -7.29 Â± 0.016 |
| P-EMB (dw) | -7.39 Â± 0.008 | -7.34 Â± 0.009 | **-7.28 Â± 0.014** |
| AP-EMB | -7.74 Â± 0.003 | -7.79 Â± 0.003 | -7.91 Â± 0.003 |
| HPF | -7.78 Â± 0.004 | -7.79 Â± 0.004 | -7.77 Â± 0.003 |
| Poisson PCA | -7.29 Â± 0.011 | -7.25 Â± 0.013 | **-7.22 Â± 0.013** |

### MovieLens Dataset

| Model | K=10 | K=20 | K=50 |
|-------|------|------|------|
| P-EMB | -2.814 Â± 0.006 | -2.794 Â± 0.006 | -2.781 Â± 0.007 |
| P-EMB (dw) | **-1.297 Â± 0.002** | **-1.297 Â± 0.002** | **-1.298 Â± 0.002** |
| AP-EMB | -2.154 Â± 0.006 | -2.151 Â± 0.006 | -2.150 Â± 0.006 |
| HPF | -0.028 Â± 0.000 | -0.023 Â± 0.000 | -0.025 Â± 0.000 |
| Poisson PCA | -0.027 Â± 0.000 | -0.033 Â± 0.000 | -0.043 Â± 0.000 |

### Key Findings

- âœ… **Multiplicative models (P-EMB) outperform additive variants (AP-EMB)**
- âœ… **Downweighting zeros improves performance on sparse data**
- âœ… **AP-EMB limited by non-negativity constraint** â€” cannot model negative correlations

## ğŸ–¼ï¸ Embedding Visualizations

<p align="center">
  <img src="results/figures/tsne_comparison.png" alt="t-SNE Visualization" width="800"/>
</p>

P-EMB-DW produces the clearest cluster separation, while AP-EMB shows degenerate patterns due to non-negativity constraints.

## ğŸ“š Reference

If you use this code, please cite the original paper:

```bibtex
@inproceedings{rudolph2016exponential,
  title={Exponential Family Embeddings},
  author={Rudolph, Maja and Ruiz, Francisco J. R. and Mandt, Stephan and Blei, David M.},
  booktitle={Advances in Neural Information Processing Systems},
  volume={29},
  pages={478--486},
  year={2016}
}
```

## ğŸ™ Acknowledgments

- Original paper: [Rudolph et al. (2016) - Exponential Family Embeddings](https://papers.nips.cc/paper/2016/hash/7b7a53e239400a13bd6be6c91c4f6c4e-Abstract.html)
- [PyTorch](https://pytorch.org/) for deep learning framework
- [UMAP](https://umap-learn.readthedocs.io/) for dimensionality reduction

---
