# Exponential Family Embeddings - Reproduction Study

A PyTorch reproduction of **Exponential Family Embeddings (EFE)** from Rudolph et al. (2016), focusing on Poisson-based models for count data applications.

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## ğŸ“– Overview

This project reproduces the Exponential Family Embeddings framework for learning distributed representations from count data. We implement three Poisson embedding variants and evaluate them on market basket and movie rating datasets.

### Models Implemented

| Model | Description | Mean Function |
|-------|-------------|---------------|
| **P-EMB** | Poisson Embedding (multiplicative) | Î» = exp(Ïáµ€á¾±) |
| **P-EMB-DW** | P-EMB with downweighted zeros (wâ‚€=0.1) | Î» = exp(Ïáµ€á¾±) |
| **AP-EMB** | Additive Poisson Embedding | Î» = Ïáµ€á¾± + b |
| **HPF** | Hierarchical Poisson Factorization (baseline) | Î» = Î¸áµ¤áµ€Î²áµ¢ |
| **Poisson PCA** | Poisson PCA (baseline) | Î» = exp(wáµ€h + c + Î¼) |

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/exponential-family-embeddings.git
cd exponential-family-embeddings

# Create virtual environment (optional but recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Requirements

```txt
torch>=2.0.0
numpy>=1.21.0
pandas>=1.3.0
scikit-learn>=1.0.0
matplotlib>=3.5.0
seaborn>=0.11.0
tqdm>=4.62.0
umap-learn>=0.5.0
```

### Run on Google Colab

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yourusername/exponential-family-embeddings/blob/main/notebooks/Exponential_Family_Embeddings.ipynb)

1. Open the notebook in Google Colab
2. Go to `Runtime â†’ Change runtime type â†’ GPU`
3. Run all cells

## ğŸ“ Project Structure

```
exponential-family-embeddings/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ LICENSE
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ Exponential_Family_Embeddings.ipynb    # Main notebook (Colab-ready)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models.py              # P-EMB, AP-EMB, HPF, Poisson PCA
â”‚   â”œâ”€â”€ data_utils.py          # Data loading and preprocessing
â”‚   â”œâ”€â”€ evaluation.py          # Normalized log-likelihood metrics
â”‚   â””â”€â”€ visualization.py       # t-SNE, UMAP plotting utilities
â”œâ”€â”€ data/
â”‚   â””â”€â”€ README.md              # Instructions for obtaining datasets
â””â”€â”€ results/
    â””â”€â”€ figures/               # Generated plots and visualizations
```

## ğŸ“Š Datasets

### Market Basket Data

The dataset should contain grocery transaction records with the following columns:

| Column | Description |
|--------|-------------|
| `product_id` | Unique product identifier |
| `household_key` | Household identifier |
| `basket_id` | Transaction/basket identifier |
| `quantity` | Purchase quantity |

### MovieLens Data

Download from [MovieLens](https://grouplens.org/datasets/movielens/):
- Ratings â‰¥ 3 are converted to counts
- Ratings < 3 are set to zero

### Data Preprocessing

```python
from src.data_utils import MarketBasketData

# Load and preprocess data
data = MarketBasketData(
    df,
    basket_col='basket_id',
    item_col='product_id',
    qty_col='quantity',
    min_item_freq=10,      # Filter rare items
    min_basket_size=2      # Remove single-item baskets
)

# Train/validation/test split
train_data, val_data, test_data = data.train_val_test_split(
    test_frac=0.05,
    val_frac=0.05
)
```

## ğŸ”§ Usage

### Training Models

```python
from src.models import PEMB_MiniBatch, PEMB_Downweighted_MiniBatch, APEMB_MiniBatch

# Initialize model
model = PEMB_Downweighted_MiniBatch(
    n_items=data.n_items,
    n_factors=50,           # Embedding dimension K
    zero_weight=0.1,        # Downweight factor for zeros
    reg=1e-6,               # L2 regularization
    device='cuda'           # Use GPU
)

# Train
model.fit(
    train_data,
    n_epochs=100,
    batch_size=2048,
    lr=0.1,                 # Adagrad learning rate
    n_neg=10,               # Negative samples per positive
    patience=10             # Early stopping patience
)

# Get embeddings
item_embeddings = model.get_item_embeddings()
context_embeddings = model.get_context_embeddings()
```

### Evaluation

```python
from src.evaluation import evaluate_model

# Evaluate on test set
metrics = evaluate_model(model, test_data, n_bootstrap=100)

print(f"Normalized Log-Likelihood: {metrics['normalized_llh']:.4f} Â± {metrics['normalized_llh_se']:.4f}")
```

### Visualization

```python
from src.visualization import plot_embeddings_umap

# UMAP visualization
plot_embeddings_umap(
    trained_models,
    train_data,
    K=50,
    n_clusters=10
)
```

## ğŸ“ˆ Results

### Market Basket Dataset

| Model | K=10 | K=20 | K=50 |
|-------|------|------|------|
| P-EMB | -7.30 Â± 0.010 | **-7.28 Â± 0.014** | -7.29 Â± 0.016 |
| P-EMB (dw) | -7.39 Â± 0.008 | -7.34 Â± 0.009 | **-7.28 Â± 0.014** |
| AP-EMB | -7.74 Â± 0.003 | -7.79 Â± 0.003 | -7.91 Â± 0.003 |
| HPF | -7.78 Â± 0.004 | -7.79 Â± 0.004 | -7.77 Â± 0.003 |
| Poisson PCA | -7.29 Â± 0.011 | -7.25 Â± 0.013 | **-7.22 Â± 0.013** |

### MovieLens Dataset

| Model | K=10 | K=20 | K=50 |
|-------|------|------|------|
| P-EMB | -2.814 Â± 0.006 | -2.794 Â± 0.006 | -2.781 Â± 0.007 |
| P-EMB (dw) | **-1.297 Â± 0.002** | **-1.297 Â± 0.002** | **-1.298 Â± 0.002** |
| AP-EMB | -2.154 Â± 0.006 | -2.151 Â± 0.006 | -2.150 Â± 0.006 |
| HPF | -0.028 Â± 0.000 | -0.023 Â± 0.000 | -0.025 Â± 0.000 |
| Poisson PCA | -0.027 Â± 0.000 | -0.033 Â± 0.000 | -0.043 Â± 0.000 |

### Key Findings

- âœ… **Multiplicative models (P-EMB) outperform additive variants (AP-EMB)**
- âœ… **Downweighting zeros improves performance on sparse data**
- âœ… **AP-EMB limited by non-negativity constraint** â€” cannot model negative correlations

## ğŸ–¼ï¸ Embedding Visualizations

<p align="center">
  <img src="results/figures/tsne_comparison.png" alt="t-SNE Visualization" width="800"/>
</p>

P-EMB-DW produces the clearest cluster separation, while AP-EMB shows degenerate patterns due to non-negativity constraints.

## ğŸ“š Reference

If you use this code, please cite the original paper:

```bibtex
@inproceedings{rudolph2016exponential,
  title={Exponential Family Embeddings},
  author={Rudolph, Maja and Ruiz, Francisco J. R. and Mandt, Stephan and Blei, David M.},
  booktitle={Advances in Neural Information Processing Systems},
  volume={29},
  pages={478--486},
  year={2016}
}
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Original paper: [Rudolph et al. (2016) - Exponential Family Embeddings](https://papers.nips.cc/paper/2016/hash/7b7a53e239400a13bd6be6c91c4f6c4e-Abstract.html)
- [PyTorch](https://pytorch.org/) for deep learning framework
- [UMAP](https://umap-learn.readthedocs.io/) for dimensionality reduction

---

<p align="center">
  Made with â¤ï¸ for reproducible research
</p>
